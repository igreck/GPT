{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6116cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-1.7B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = False, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.9, # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load imdb data\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"imdb\")\n",
    "# Keep only positive samples for SFT as  per description\n",
    "train_pos = ds[\"train\"].filter(lambda x: x[\"label\"] == 1)\n",
    "test_pos = ds[\"test\"].filter(lambda x: x[\"label\"] == 1)\n",
    "if 1000 > 0 and 1000 < len(train_pos):\n",
    "    train_pos = train_pos.select(range(1000))\n",
    "if 200 > 0 and 200 < len(test_pos):\n",
    "    test_pos = test_pos.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_pos,\n",
    "    eval_dataset= test_pos,\n",
    "    compute_metrics=\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        output_dir=\"./sft-imdb-qlora\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 100,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        save_steps=500,\n",
    "        eval_strategy=\"steps\",  # \"epoch\" is simplest\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lora(trainer, tokenizer, output_dir):\n",
    "    # This saves the PEFT adapter and trainer state\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Saved LoRA adapter and tokenizer to: {output_dir}\")\n",
    "\n",
    "save_lora(trainer, tokenizer, './sft_imdb_qlora_qwen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41938bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "\n",
    "def load_lora_for_inference(model_name, max_length, output_dir):\n",
    "    # Re-create base, then attach LoRA adapter\n",
    "    base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_length,\n",
    "        load_in_4bit=False,      # for inference you can use 4bit=True too\n",
    "        fast_inference=True,\n",
    "        gpu_memory_utilization=0.9,\n",
    "    )\n",
    "    # if tokenizer.pad_token is None:\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "    # tokenizer.padding_side = \"left\"  # for generation often left padding works well\n",
    "\n",
    "    peft_model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "    peft_model.eval()\n",
    "    return peft_model, tokenizer\n",
    "\n",
    "\n",
    "# 7) Load back for inference\n",
    "inf_model, inf_tokenizer = load_lora_for_inference(\n",
    "    model_name=\"unsloth/Qwen3-1.7B-Base\",\n",
    "    max_length=1024,\n",
    "    output_dir='./sft_imdb_qlora_qwen'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36206630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "@torch.inference_mode()\n",
    "def demo_generation(model, tokenizer, eval_ds, max_new_tokens=64, max_length=1024, demo_n=4):\n",
    "    print(\"\\n===== Demo: Generations on 3-4 reviews =====\")\n",
    "    n = min(demo_n, len(eval_ds))\n",
    "    idxs = random.sample(range(len(eval_ds)), n)\n",
    "\n",
    "    for i, idx in enumerate(idxs, start=1):\n",
    "        src = eval_ds[idx][\"text\"]\n",
    "        # Use a short prefix of the review to show continuation capability\n",
    "        prompt = src[:400]  # first 400 chars\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(model.device)\n",
    "        gen_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            # eos_token_id=tokenizer.eos_token_id,\n",
    "            # pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        out = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        print(f\"\\n--- Sample {i} ---\")\n",
    "        print(out)\n",
    "    print(\"\\n=====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_generation(model=inf_model, tokenizer=inf_tokenizer, eval_ds=test_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def batch_generate_only_new(model, tokenizer, prompts, max_new_tokens=128, **gen_kwargs):\n",
    "    device = next(model.parameters()).device\n",
    "    enc = tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    input_lens = enc[\"attention_mask\"].sum(dim=1)  # [B]\n",
    "    out = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for i, seq in enumerate(out.sequences):\n",
    "        ilen = int(input_lens[i].item())\n",
    "        gen_part = seq[ilen:]\n",
    "        text = tokenizer.decode(gen_part, skip_special_tokens=True)\n",
    "        results.append(text)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cdd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The movie was so bad, and i hate when\",\n",
    "    \"The movie was\",\n",
    "    \"Finish this thought about the acting quality:\",\n",
    "]\n",
    "for i, t in enumerate(batch_generate_only_new(inf_model, inf_tokenizer, prompts, max_new_tokens=64), 1):\n",
    "    print(f\"=== {i} ===\\n{t}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
